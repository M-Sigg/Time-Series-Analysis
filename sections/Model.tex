\subsection{Fitting}

Given an observed time series, estimate which model best approximates its dynamics:\\
\begin{itemize}
    \item Choose from $\overbrace{AR(p),MA(q),ARMA(p,q),...}^\text{Model Selection}$ and estimate parameters (e.g., $\underbrace{\alpha_1,\ldots, \alpha_p, \beta_1,\ldots,\beta_q}_\text{estimation}$).
\end{itemize}

\underline{\textbf{Inference}: Draw confidence intervals for parameters estimates} \\

\textit{Note}: Inference for the mean of a stationary time series.\\

Let $X_t$ be a stationary time series. This means $\mathbb{E}[X_t]=\mathbb{E}[X_{t+1}]=\mathbb{E}[X_{t+2}]=\ldots=\mu $. So $\mu$ is "population unconditional mean". Let $\gamma_k$ be the $k^{-th}$-order population autocovariance
\[
\gamma_k=Cov(X_t,X_{t-k})
\]
We want to determine the confidence interval for $\mu$ given $X_1,\ldots,X_T$

\begin{figure}[H]
\includegraphics[scale=0.4]{images/Screenshot 2024-03-31 at 16.34.09.jpg}
\centering
\end{figure}

Regular stats confidence interval (margin of error measure) is calculated as $\hat{\mu}=\frac{1}{T}\sum_{t=1}^T X_t=\bar{X}_T$\\

We want to construct a confidence interval $[a,b]$ such that $Pr(\mu$ in [a,b]$) \geq 95\%$ \\

The \underline{strategy} to achieve this involves several steps. First, calculate $\text{var}(\hat{\mu}) = \text{var}(\bar{X}_T)$. By the Central Limit Theorem, $\hat{\mu}$ is approximately Gaussian. We can then use the 95\% quantiles of the Gaussian distribution with variance $\text{var}(\hat{\mu})$ to produce a formula that can be coded and implemented on a computer.\\

Fact: When using sample analogues $\hat{\gamma}_k$, it is important not to use all of them for the sum due to the degrees of freedom problem:
\begin{align*}
    \widehat{\text{var}}(\hat{\mu}) &= \frac{1}{T^2} \sum_{t=1}^T \sum_{s=1}^T \text{Cov}(X_t, X_s) \\
    &= \frac{1}{T^2} \left( \sum_{t=1}^T \text{Var}(X_t) + \sum_{t=1}^T \sum_{\substack{s=1 \\ s \neq t}}^T \text{Cov}(X_t, X_s) \right) \\
    &= \frac{\gamma_0}{T} + \frac{1}{T^2} \sum_{k=1}^{T-1} \gamma_k(T - k)
\end{align*}

This results in a quadratic increase in the number of terms. However, as the lag increases, the number of pairs available for covariance calculation decreases, leading to fewer degrees of freedom and eventually, it vanishes.\\

To address this \textbf{degrees of freedom problem} we truncate the sum at a given lag $h$, where $l \ll h \ll T$. A common choice for $h$ is $\lfloor \sqrt{T} \rfloor$ or $\lfloor \log T \rfloor$. The truncated variance estimate is:

\[
\widehat{\text{var}}(\hat{\mu}) = \frac{\gamma_0}{T} + \frac{1}{T^2} \sum_{k=1}^h \gamma_k (T - k).
\]
Then, the confidence interval is given by:
\[
\text{C.I.} [a, b] = \hat{\mu} \pm 1.96 \sqrt{\widehat{\text{var}}(\hat{\mu})}.
\]
This estimator is sometimes called the "HAC" estimator, which stands for heteroskedasticity and autocorrelation consistent. \\

To further elaborate, the variance of $\hat{\mu}$ is:
\[
\text{var}(\hat{\mu}) = \text{var}\left(\frac{1}{T} \sum_{t=1}^T X_t\right).
\]
This is difficult to expand if $\text{Cov}(X_t, X_s) \neq 0$. Thus, we write it as:
\[
\text{var}(\hat{\mu}) = \frac{1}{T^2} \sum_{t=1}^T \text{var}(X_t) + \frac{1}{T^2} \sum_{s \neq t} \text{Cov}(X_t, X_s).
\]

In the case of iid data, we would estimate $\widehat{\text{var}}(\hat{\mu}) = \frac{1}{T} \sum_{t=1}^T (X_t - \hat{\mu})^2 = \gamma_0$. The confidence interval is then formed as:
\[
\hat{\mu} \pm 1.96 \times \frac{\sqrt{\hat{\gamma_0}}}{\sqrt{T}}.
\]


For time series data, one can form $\hat{\gamma}_k$ for all $k$ and use them carefully. \\

The \textbf{Heteroskedasticity and Autocorrelation Consistent (HAC) Estimators} are used to provide robust standard errors in the presence of heteroskedasticity and autocorrelation in the error terms of a regression model. These estimators adjust the covariance matrix of the parameter estimates to account for the fact that the error term may not be identically distributed (heteroskedasticity) and may be correlated across time (autocorrelation).

\textbf{Purpose of HAC Estimators}

HAC estimators are primarily used for the following purposes:
\begin{enumerate}
    \item \textbf{Correcting Standard Errors:} In the presence of heteroskedasticity and autocorrelation, the usual OLS standard errors can be biased and inconsistent. HAC estimators correct for these issues, providing more accurate standard errors.
    \item \textbf{Reliable Inference:} With corrected standard errors, the confidence intervals and hypothesis tests based on these estimates are more reliable. This is crucial for making valid statistical inferences about the parameters of the model.
\end{enumerate}
The bandwidth (or lag) in HAC estimators determines how many lags of the autocorrelation are considered when estimating the covariance. The choice of bandwidth affects the robustness of the standard errors.

\subsection{PACF}

\underline{Partial Autocorrelation Function}, and choosing between $AR(p_1)$ and $AR(p_2)$ for 2 different Lags. \\

\textbf{\underline{Definition}}: \quad The function $PACF(k)$ is defined by the estimate $\pi_k$ in the least squares fit: \[
X_t=\nu +\pi_1 X_{t-1} +...+ \pi_k X_{t-k}
\]
In stats software packages, ---- bounds are drawn at $\pm \frac{2}{\sqrt{T}}$ like for auto correlation functions when properly normalized. This is done with unit variance scaling of $X_t$ (means replace $X_t$ with $\frac{X_t}{var(X_t)}=\frac{X_t}{\sigma^2}$)(Software does this automatically)

\begin{itemize}
    \item The PACF essentially isolated the direct effect of lag $k$ on the correlation, removing the influence of intermediate lags.
    \item This helps identify the order of AR terms in ARMA models. Significant spikes in the PACF at specific lags indicate potential AR components. A cut-off after lag $p$ in the PACF suggests an $AR(p)$ model might be appropriate
\end{itemize}

\subsection{Estimation}

\subsubsection{Estimating Parameters of an AR Process}

Given an AR(p) process: \[
X_t -\mu = \alpha_1(X_{t-1}-\mu) + \cdots + \alpha_p (X_{t-p} -\mu) Z_t
\] Given $N$ observations $x_1, \ldots, x_N$, the parameters $\mu, \alpha_1, \ldots, \alpha_p$ may be estimated by least squares by minimizing\[
S = \sum_{t=p+1}^N \left[x_t-\mu-\alpha_1(x_{t-1}-\mu) - \cdots - \alpha_p (x_{t-p}-\mu) \right]^2
\] with respects to $\mu, \alpha_1, \ldots, \alpha_p$. If the $Z_t$ process is normal, then this is equivalent to maximum likelihood estimation. \\

In the first order case we get: \[
\hat{\mu} = \frac{\bar{x}_{(2)} - \hat{\alpha}_1 \bar{x}_{(1)}}{1-\hat{\alpha}_1}
\] and \[
\hat{\alpha}_1 = \frac{\sum_{t=1}^{N-1} (x_t-\hat{\mu})(x_{t+1}-\hat{\mu}) }{\sum_{t=1}^{N-1} (x_t- \hat{\mu})^2}
\] where $\bar{x}_{(1)},\bar{x}_{(2)} $ are the means of the first and last $(N-1)$ observations. Now \[ 
\bar{x}_{(1)} \simeq \bar{x}_{(2)} \simeq \bar{x}
\] thus we have approximately \[
\hat{\mu} = \bar{x}
\] Then the estimator becomes \[
\hat{\alpha}_1 \approx \frac{\sum_{t=1}^{N-1} (x_t-\bar{x})(x_{t+1}-\bar{x}) }{\sum_{t=1}^{N-1} (x_t- \bar{x})^2}
\]
This is basically the same estimator as in an ordinary regression with $X_{t-1}-\bar{x}$ as the 'independent' variable. In fact, asymptotically, much of classical regression theory can be applied to AR models. \\
Further approximation, can be used by changing the denominator to \[\sum_{t=1}^N(x_t-\bar{x})^2\] So that \begin{align*}
    \hat{{\alpha}}_1 &\simeq \frac{c_1}{c_0}\\
    &= r_1
\end{align*} Where $r_1$ is an estimator for $\rho(1)$. \\

In a second order case, we'd get \begin{align*}
    \hat{\mu} &\simeq \bar{x}\\
    \hat{\alpha}_1 &\simeq r_1(1-r_2)/(1-r_1^2) \\
    \hat{\alpha}_2 &\simeq (r_2-r_1^2)/(1-r_1^2)
\end{align*} Where $\hat{\alpha}_2$ is the partial autocorrelation coefficient of order two. 

\subsubsection{Fitting a MA Process}
Estimation is more difficult for an MA process than an AR process, because some form of numerical iteration must be performed. First, consider a MA(1) process: \begin{align}
X_t = \mu + Z_t + \beta_1 Z_{t-1} \label{4.13}
\end{align}
Unfortunately, we cannot write the residual sum of squares $\Sigma Z_t^2$ solely in terms of the observed $x$s and the parameters $\mu, \beta_1$ and so no explicit least squares estimates can be found. One possible alternative is to (i) Select suitable starting values for $\mu$ and $\beta_1$, such as $\mu=\bar{x}$ and the value of $\beta_1$ given by the solution to this equation: $r_1 = \hat{\beta}_1/ (1+\hat{\beta}_1^2)$. (ii) Calculate the corresponding residual sum of squares using Equation (\ref{4.13}) recursively in the form \[ Z_t = X_t -\mu -\beta_1 Z_{t-1}\] Taking $z_0 = 0$, we get $z_1 = x_1 -\hat{\mu}$, and then $z_2 = x_1 -\hat{\mu}-\hat{\beta}_1 z_1$, and so on until $z_N = x_N- \hat{\mu}- \hat{\beta}_1 z_{N-1}$. Then the residual sum of squares $\sum_{t=1}^N z_t^2$ is calculated conditional on the given values of the parameters $\mu$ and $\beta_1$ and $z_0=0$. (iii) Repeat this procedure for other neighbouring values of $\mu$ and $\beta_1$ so that the residual sum of squares $\Sigma z_t^2$ is computed on a grid of points in the $(\mu,\beta_1)$ plane. (iv) Determine by inspection the values of $\mu$ and $\beta_1$ that minimize $\Sigma z_t^2$.


\subsubsection{Estimating Parameters of an ARMA Model}
The estimation problems are similar to those for an MA model in that an iterative procedure has to be used. As an example consider the ARMA(1,1) model: \[
X_t - \mu = \alpha_1 (X_{t-1}- \mu) + Z_t + \beta_1 Z_{t-1}
\]Given $N$ observations, $x_1, \ldots, x_N$, we guess values for $\hat{\mu}, \hat{\alpha}_1, \hat{\beta}_1 $, set $z_0 = 0$ and $x_0 = \hat{\mu}$, and then calculate the residuals recursively by \begin{align*}
    z_1 &= x_1 - \hat{\mu} \\
    z_2 &= x_2-\hat{\mu} - \hat{\alpha}_1 (x_1-\mu) - \hat{\beta}_1 z_1 \\
    &\text{ up to} \\
    z_N&= x_N - \hat{\mu}- \hat{\alpha}(x_(N-1) -\mu) - \hat{\beta}z_{N-1}
\end{align*}
The residual sum of squares $\sum_{t=1}^N z_t^2$ may then be calculated. Then other values of $\hat{\mu}, \hat{\alpha}_1, \hat{\beta}_1 $ may be tried until the minimum residual sum of squares is found.


\rule{\textwidth}{0.4pt}

\subsubsection{Estimation: Lecture}


Least squares estimation for $\varepsilon_t$ estimates. Note that least squares is equivalent to Maximum Likelihood for $\varepsilon_t$ Gaussian \\

For general $ARMA(p,q)$ model:

$ARMA(1,0)$: \[
X_t=\mu+\alpha X_{t-1} + \varepsilon_t+\beta\varepsilon_{t-1}
\]
\begin{itemize}
    \item Assume $\beta=0$ to start easy
    \item[] Given candidate $\alpha, \mu$ call it $\alpha^{\textit{candidate}}, \mu^{\textit{cand.}}$ Form: \[\hat{\varepsilon}(\alpha^{\textit{cand.}},\mu^{\textit{cand.}}) = X_t -\mu^{\textit{cand.}} - \alpha^{\textit{cand.}}X_{t-1}
    \]
    \item[] Calculate: \[SS(\alpha^{\textit{cand.}},\mu^{\textit{cand.}})=\sum_{t=2}^T \hat{\varepsilon}_t(\alpha^{\textit{cand.}},\mu^{\textit{cand.}} )^2 \]
    \item[] Take $\hat{\alpha}, \hat{\mu}$ to be minimizer of $SS(\alpha^{\textit{cand.}},\mu^{\textit{cand.}})$
\end{itemize}

Straight generalization to $MA(q)$ or $ARMA(p,q)$ etc. is \underline{not} straight forward.\\

\textbf{With MA}
\begin{itemize}
    \item $X_t=\mu+\varepsilon_t+\theta\varepsilon_{t-1}$ $\leftarrow$ Hard to do the instinctive thing, e.g.,
    \begin{itemize}
        \item[] $X_t-\mu-\varepsilon_t-\theta\varepsilon_{t-1}=0$
        \item[] $\underbrace{\varepsilon_t=X_t-\mu-\theta\varepsilon_{t-1}}_\text{Cannot square these and sum, because $\varepsilon_{t-1}$ is on inside.}$
        \begin{itemize}
            \item[] $\rightarrow$ Don't have an ex ante estimate $\hat{\varepsilon}_{t-1}$ \underline{unless} we approximate ("initialize") $\hat{\varepsilon}_0=0$
        \end{itemize}
    \end{itemize}
    \item For candidate $\mu^{\textit{cand.}}$, $\theta^{\textit{cand.}}$
    \begin{itemize}
        \item[]
        \begin{align*}
            \hat{\varepsilon}_1&=X_1-\mu^{\textit{cand.}}-\theta^{\textit{cand.}}\cdot \underbrace{\hat{\varepsilon}_0}_{=0}\\
            &=X_1-\mu^{\textit{cand.}}\\
        \end{align*}
        \item[]$\Rightarrow$ $\hat{\varepsilon}_1$ can be calculated now
        \begin{align*}
            \hat{\varepsilon}_2=X_2-\mu^{\textit{cand.}}-\theta^{\textit{cand.}} \cdot \hat{\varepsilon}_1
        \end{align*}
        \item[]$\Rightarrow$ $\hat{\varepsilon}_2$ can be calculated 
        \item[] etc.
    \end{itemize}
    \item[] Let $\hat{\mu}, \hat{\theta}$ $\underset{\mu^{\text{cand.}},\theta^{\text{cand.}}}{\text{minimize}}$ $\sum_{t=1}^T \hat{\varepsilon}_t^2$
    \begin{itemize}
        \item[] For finite order $MA(q)$, bias $\sim \frac{1}{T}$ is "small"
        \item[] ! This is smaller than $\frac{1}{\sqrt{T}}$, the statistical margin of error for $\hat{\mu}, \hat{\theta}$
    \end{itemize}
\end{itemize}

\textbf{General $ARMA(p,q)$ model:} Least Squares Approach
\[
X_t=\mu+\alpha_1X_{t-1}+...+\alpha_p X_{t-p}+\varepsilon_t+\theta_1 \varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}
\]
\begin{itemize}
    \item Initialize $\hat{\varepsilon}_0, \hat{\varepsilon}_{-1},...,\hat{\varepsilon}_{-q+1}=0 $
    \item[$\Rightarrow$] can calculate $\hat{\varepsilon}_1,\hat{\varepsilon}_2,...,\hat{\varepsilon}_T $ recursively, given candidate parameters
    \begin{itemize}
        \item Then minimize $\sum_{t=1}^T\hat{\varepsilon}_t^2 $
        \begin{itemize}
            \item[$\rightarrow$] $\hat{\mu},\hat{\alpha}_1,...,\hat{\alpha}_p, \hat{\theta}_1,...,\hat{\theta}_q $
        \end{itemize}
    \end{itemize}
    \item[] Can also minimize over $\hat{\varepsilon}_0$, computationally difficult 
\end{itemize}
\fbox{Maximum Likelihood Estimation} \quad (Alternative to Least Squares)
\begin{itemize}
    \item Native in Software
    \item Equivalent to Least Squares if $\varepsilon_t \sim N(0,\sigma^2)$, (i.e., $\varepsilon_t$ are Gaussian)
    \item Flexible enough that it can allow non-Gaussian $\varepsilon_t$ by appropriate adjustment to Likelihood
\end{itemize}

\subsection{Model Selection}

\begin{itemize}
    \item[] For $ARMA(p,q)$: \quad what should $p,q$ be?
    \item[] For $ARIMA(p,d,q)$: \quad what should $p,d,q$ be?
\end{itemize}
Guiding principle: Take $p+q$ (or $p+d+q$) to be as small as possible while still modeling the data well
\begin{itemize}
    \item ACF and PACF of $\hat{\varepsilon}_t$ are zero (to statistical precision)
    \item Likelihood small relative to marginal benefit from the next parameter
\end{itemize}

\textbf{\underline{Tests for leftover correlation in residuals}} \\

When a model has been fitted to a time series, it is advisable to check that the model really does provide an adequate description of the data. This is usually done by looking at the residuals. If we have a 'good' model, we expect the residuals to be 'random' and 'close to zero'. Two obvious steps are to plot the residuals as a time plot, and to calculate the correlogram of the residuals.


\[
X_t=\hat{\mu} +\hat{\alpha}_1X_{t-1}+...+\hat{\alpha}_p X_{t-p}+\hat{\varepsilon}_t +\hat{\theta}_1\hat{\varepsilon}_{t-1}+...+\hat{\theta}_q \hat{\varepsilon}_{t-q}
\]
\begin{itemize}
    \item Look at ACF, PACF of $\hat{\varepsilon}_t$ against $\pm \frac{2}{\sqrt{T}}$ limits drawn by Software
    \item Ljung-Box test: For $\hat{r}_{\varepsilon,k}^2=$ autocorr. of $\hat{\varepsilon} \log k$
    \begin{align*}
        Q&=T(T+2) \sum_{k=1}^K \frac{\hat{r}_{\varepsilon,k}^2}{T-k}\\
        Q &\sim \chi^2_{K-p-q} &&\text{d.f. $K$ choice, say $K=10$ e.g.}
    \end{align*}
    \begin{itemize}
        \item[] $H_0$: no serial correlation in residuals
    \end{itemize}
    \item Durbin-Watson test:
    \begin{align*}
        d&=\frac{\sum(\hat{\varepsilon}_t-\hat{\varepsilon}_{t-1})^2}{\sum (\hat{\varepsilon}_t^2)} \sim \text{ "DW", "Special reference dist."} \\
        &\approx 2(1-\hat{r}_{\varepsilon,2})
    \end{align*}
\end{itemize}

\textbf{\underline{Information Criteria}}
\begin{itemize}
    \item Akaike Information Criteria:
    \begin{itemize}
        \item $AIC=-2\cdot log\ Likelihood+2\cdot \# \ parameters$
    \end{itemize}
    \item Bias corrected Akaike Information Criteria 
    \begin{itemize}
        \item $AIC_c=-2\cdot log\ Likelihood + \frac{T\cdot 2\cdot \# parameters}{T-\#parameters-1}$
    \end{itemize}
    \item Bayesian Information Criteria:
    \begin{itemize}
        \item $BIC=-2\cdot log\ Likelihood+log\ T\cdot \# parameters$
    \end{itemize}
    \item[] Small $AIC$/$AIC_C$/$BIC$ preferred
    \begin{itemize}
        \item AIC under-penalizes relative to BIC:
        \begin{itemize}
            \item helpful for confidence intervals (for example $\hat{\alpha}_1\pm $ margin of error)
            \item Called under-smoothing - hoping less bias
        \end{itemize}
        \item BIC generally better for forecasting, $log\ T$ multiplier cancels typical boost in likelihood from added parameters
    \end{itemize}
\end{itemize}

