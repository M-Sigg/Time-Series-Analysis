Technically, no official definition of nonlinear time series exists, but sometimes, there is an obvious choice for what to call linear or nonlinear. \\

For a time series like \[X_t = f(X_{t-1} ... X_{t-p}) + \varepsilon_t \]
\begin{itemize}
    \item if $f(X_{t-1} ... X_{t-p})$ is something like $ =\alpha_1 X_{t-1} + ...+ \alpha_p X_{t-p}$ call this linear. Otherwise, nonlinear.
\end{itemize}
ARMA models are considered linear by everyone. 

\subsection{Why non-linearity?}

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{plots/sunpots.pdf}
\caption{Average monthly sunspot numbers from 1860 to 1983. The lower graph has a shortened vertical axis, which enables the rise and fall of the graph to be more easily assessed.}
\end{figure}

Graph shows there is regular cyclic behavior with a period of approximately 11 years. But we also see that the series generally increases at a faster rate than it decreases. \textbf{Behavior like this cannot be explained by a linear model}.\
"Such asymmetric behavior can also arise in studying the economy since the relationship between economic variables tend to be different when the economy is moving into recession rather than when coming out of one" (p.268). \\

"For seasonal series with a fixed cycle length, it may be possible to model asymmetric behavior with a non-sinusoidal seasonal component, but when the cycle length is not fixed, a non-linear model is much more compelling for describing series with properties such as 'going up faster than coming down'" (p.269).\\

Before starting non-linear analysis, it is sensible to ensure that the data really is non-linear. But tests for non-linearity often have poor power, and the simplest, and arguably the most important tool is a careful inspection of the time plot. 


\subsection{Threshold AR models}
A \textbf{threshold AR (TAR) model} is a piecewise linear model where the parameters of an AR model are determined by the values by the values taken by one or more of the lagged values of the time series. \\

Idea: Glue together two different AR models.\\

Simplest way is depending on value of $X_{t-1}$:
\begin{subequations}
\begin{empheq}[left = {X_t=\empheqlbrace}]{align*} 
    &v_1 + a_1X_{t-1} + \sigma_1 \varepsilon_t &&\text{if $X_{t-1} < r$} \\
    &v_2 + a_2 X_{t-1} + \sigma_2 \varepsilon_t &&\text{if $X_{t-1}\geq r$}
\end{empheq}
\end{subequations}
The AR parameter depends on whether $X_{t-1}$ exceeds the value $r$, called the \textbf{threshold}.


\begin{figure}[h]
\includegraphics[scale=0.4]{images/Screenshot 2024-05-06 at 08.49.15.jpg}
\centering
\end{figure}


Estimation: Choose a distribution for $\varepsilon_t$ (like $N(0,1)$) and do maximum likelihood. \[\rightarrow \hat{v}_1,\hat{v}_2,\hat{\alpha}, \hat{\alpha}_2, \hat{\sigma}_1, \hat{\sigma}_2, \hat{r}  \]

\subsection{Volatility Models}

Whereas the previous non-linear model allows for improved point forecasts of the observed variable to be made when the true model is known, we here focus on \textbf{volatility models} that are primarily concerned with modelling \textit{changes in variance} or \textit{volatility}. They do not generally lead to better point forecasts, but may lead to better estimates of the (local) variance. This, in turn, allows more reliable prediction intervals and hence a better assessment of risk. 

\begin{figure}[H]
\includegraphics[scale=0.4]{images/Screenshot 2024-05-13 at 09.11.00.jpg}
\centering
\end{figure}


There are swells in the variance of the time series. \\
ARMA models cannot capture this behavior. \\

Suppose: try modeling Daily Microsoft returns with a fixed variance model.
\begin{itemize}
    \item Might use efficient markets: $E[X_t]=0$
    \item That leaves \[
    X_t =\sigma \cdot \varepsilon_t \quad \text{with $\varepsilon_t$ iid. and $var(\varepsilon_t)=1$.}
    \]
    \item What if someone asks: "How much risk is there in microsoft returns tomorrow?
    \begin{itemize}
        \item Observe $X_t \quad t=1,...,T$
        \item Question is about $var(X_{T+1})$ essentially. Might start by asking, what is an estimate $\hat{var}(X_{T+1})$
        \item But if you estimate $\hat{\sigma}$ in a model $X_t=\sigma \varepsilon_t$ \begin{itemize}
            \item Then $\hat{Var}(x_{T+1})=\hat{\sigma}^2$ regardless of $T$ or recent history.
        \end{itemize}
    \end{itemize}
\end{itemize}


\subsubsection{ARCH Models}
\underline{A}uto\underline{r}egressive \underline{C}onditional \underline{H}eteroskedasticity \quad [first-order case]

\[X_t= \sigma_t \varepsilon_t\]
\begin{itemize}
    \item $\varepsilon_t$ mean-zero, purely random (iid.) process with $var(\varepsilon_t)=1$
    \item $\sigma_t^2=\gamma +\alpha X_{t-1}^2$ \quad (the variance depends on the most recent value of the derived series) 
    \begin{itemize}
        \item $\sigma_t^2 = \gamma + \sum_{j=1}^k \alpha_j X_{t-j}^2$ [General form]
    \end{itemize}
\end{itemize}

\noindent 
Comments: 
\begin{itemize}
    \item oddly named, because $\sigma_t^2$ "regressed" on past $X_{t-1}$ rather than $\sigma_{t-1}^2$
    \item Compactly written $X_t=\sqrt{\gamma+ \alpha X_{t-1}^2}\varepsilon_t$
    \item There is no error term in $\sigma_t^2$ equation.
    \item Unconditional mean is zero: $E(X_t)=E(E(X_t|F_{t-1})) = E(\sigma_t E(\varepsilon_t))=0 $
\begin{itemize}
    \item Derivation:
\end{itemize}
    \begin{align*}
        Var(X_t)&=E(X_t^2) = E(\gamma + \alpha X_{t-1}^2) = \gamma + \alpha E(X_{t-1}^2) \\
        \text{As $X_t$ is stationary: } Var(X_t)&=Var(X_{t-1})=E(X_{t-1}^2) \\
        \Rightarrow Var(X_t)&= \gamma + \alpha Var(X_t) \\
        1&= \frac{\gamma}{Var(X_t)} + \alpha \\
        Var(X_t)&=\frac{\gamma}{1-\alpha}
    \end{align*}
    \item The process is stationary (extended infinitely forward and backward)
    \[var(X_t)=\frac{\gamma}{1-\alpha} \quad \text{(unconditional variance)}\]
    \item If $\alpha>0$, large $X_{t-1}^2 \Rightarrow$ large $\sigma_t^2$
    \begin{itemize}
        \item[] $\Rightarrow$ average $\sigma_t^2$ is larger than if $\alpha=0$ were true
        \item[] $\Rightarrow$ Larger conditional variance 
    \end{itemize}
    \item Example: $\alpha=\frac{1}{2} \Rightarrow var(X_t)=\frac{\gamma}{1-\frac{1}{2}} = 2\gamma> \gamma$ 
\end{itemize}
One nice property of ARCH models is that the tail distribution of $X_t$ is heavier than that of a normal distribution. Hence the innovation process $X_t$ in a Gaussian ARCH(1) model tends to generate more 'outliers' than a Gaussian white noise process.\\

\noindent
Parameters in ARCH models can be estimated by maximizing their likelihood. \\

\noindent
\underline{Book (not covered in lecture)} \\
\noindent
Consider an ARCH($k$) model. At forecast origin $N$, the 1-step-ahead forecast for $\sigma_{N+1}^2$ is \[
\sigma_N^2 (1)=E(\sigma_{N+1}^2| \mathcal{F}_N) = \gamma + \alpha_1 X_N^2 + \cdots + \alpha_k X_{N+1-k}^2
\]
and the $l$-step-ahead forecast for $\sigma_{N+l}^2$ is \[
\hat{\sigma}_N^2(l)= E(\sigma_{N+l}^2| \mathcal{F}_N) = \gamma + \sum_{i=1}^k \alpha_i \hat{\sigma}_N^2 (l-k)
\]

\subsubsection{GARCH Models}
The ARCH model has been generalized to allow the variance to depend on the past values of $\sigma_t^2$ as well as on past values of $X_t^2$.

Limitation of ARCH is $\sigma_t^2$ process resets if $\varepsilon_t \approx0$. 

\[X_t=\sigma_t \varepsilon_t\]
\begin{itemize}
    \item ARCH: $\sigma_t^2=\gamma+\alpha X_{t-1}^2$
    \item GARCH: $\sigma_t^2= \gamma+\alpha X_{t-1}^2 + \beta \sigma_{t-1}^2 $
    \begin{itemize}
        \item[] ("Autoregressive" process in $X_{t-1}^2$ and $\sigma_{t-1}^2$)
        \item Generalized: $\sigma_t^2 =\gamma + \sum_{j=1}^h \beta_j \sigma_{t-j}^2 + \sum_{i=1}^k \alpha_i X_{t-i}^2$
    \end{itemize}
\end{itemize}

Note: Alternative approach is \[ARCH(p): \sigma_t^2 = \gamma +\alpha_1 X_{t-1}^2 + \alpha_2 X_{t-2}^2 + \cdots + \alpha_p X_{t-p}^2\]
\begin{itemize}
    \item[]$\Rightarrow$ makes it harder for $\sigma_t^2$ to reset because it would need many $\varepsilon_t\approx 0$ in a row
    \item One advantage of GARCH over ARCH(p) is less parameters ($3 < p+1$ if $p$ is large)
\end{itemize}

Invertibility and stationarity assumptions of ARMA models apply to GARCH models. 

\textbf{\underline{How to estimate?}}
\begin{itemize}
    \item Assume a distribution for $\varepsilon_t$ (usually $\varepsilon_t \sim N(0,1)$)
    \item Perform maximum likelihood estimation.
    \item Jargon: if you assume $\varepsilon_t \sim N(0,1)$ in estimation but you think $\varepsilon_t \sim $ something else (e.g. $\varepsilon_t \sim t_k$) then this procedure called "Quasi Maximum Likelihood"
\end{itemize}

\subsection{Non-linear Deterministic Models}
Can generate a fully random time series $X_t$ by: 
\begin{itemize}
    \item Choosing $X_1$, randomly
    \item Choosing $X_t=f(X_{t-1})$ for $t\geq 2$ and $f$ a fixed function.
\end{itemize}

Example:
\[
f(x)=k\cdot x\cdot (1-x)
\]
\begin{itemize}
    \item If $X_1 \sim Unif[0,1]$, then $X_2$ is random and $X_2=k\cdot X_1 \cdot (1-X_1)$
    \item What is the distribution of $x_2$?
\end{itemize}


\subsection{Exercises}

\begin{footnotesize}

\subsubsection*{2. Stationarity for GARCH models}

\textbf{Question:} Consider the GARCH(h,k) model:
\begin{align*}
    X_t &= \mu+Y_t \\
    Y_t &= \sigma_t \epsilon_t, \quad \epsilon_t \overset{i.i.d.}{\sim} \mathcal{N}(0,1) \\
    \sigma^2_t &= \omega + \sum_{i=1}^h \beta_i \sigma_{t-i}^2 + \sigma_{j=1}^k \alpha_j Y_{t-j}^2, \quad \omega,\beta_i, \alpha_j \geq 0
\end{align*}
Derive stationarity conditions for this model by following these steps:
\begin{enumerate}
    \item Calculate $\mathbb{E}[Y_t]$
    \item Calculate $Cov(Y_t,Y_s)$
    \item Calculate $\mathbb{E}[\sigma_t^2]$
\end{enumerate}
\textbf{Answer:} \\

Stationarity (weak) conditions:
\begin{itemize}
    \item $\mathbb{E}[X_t] =$ constant
    \item $Var(X_t) < \infty$, constant
    \item $\gamma(\cdot)$ well defined
\end{itemize}

\textbf{1.}
\begin{align*}
    \mathbb{E}[X_t] &= \mu + \mathbb{E}[Y_t] = \mu + \mathbb{E}[\sigma_t \epsilon_t] \\ &= \mu+ \mathbb{E}[\mathbb{E}[\sigma_t \epsilon_t | \mathcal{F}_{t-1}]]\\
    &= \mu + \mathbb{E}[\sigma_t \underbrace{\mathbb{E}[\epsilon_t|\mathcal{F}_{t-1}]}_{=0}] = 0 + \mu\\ &= \mu
\end{align*}

\textbf{2.}
\begin{align*}
    \mathbb{E}[(X-t -\mu)(X_s-\mu)] &= \mathbb{E}[Y_t Y_s]\\
    &=\mathbb{E}[\sigma_t \epsilon_t \sigma_s \epsilon_s]\\
    &= \mathbb{E}[\mathbb{E}[ \sigma_t \sigma_s \epsilon_t \epsilon_s | \mathcal{F}_s]] \\
    \text{for $t>s$ } &= \mathbb{E}[\sigma_s \epsilon_s \mathbb{E}[\sigma_t\epsilon_t|\mathcal{F}_s]] \\
    \text{for $t-1 > s$ } &= \mathbb{E}[\sigma_s \epsilon_s \mathbb{E}[ \mathbb{E}[\sigma_t \epsilon_t|\mathcal{F}_{t-1} ]|\mathcal{F}_s]] = 0
\end{align*}

\begin{subequations}
\begin{equation}
\begin{array}{r@{}l}
\gamma(k) = & \left\{
\begin{aligned}
    & ?  &&k=0 \nonumber \\
    & 0 && \text{else} \nonumber
\end{aligned}
\right.
\end{array}
\end{equation}
\end{subequations}

\textbf{3.}
\begin{align*}
    Var(X_t)&= \mathbb{E}[Y_t^2] = \mathbb{E}[\sigma_t^2 \epsilon_t^2]\\
    &=\mathbb{E}[\mathbb{E}[\sigma_t^2 \epsilon_t^2| \mathcal{F}_{t-1}]]\\
    &= \mathbb{E}[\sigma_t^2 \underbrace{\mathbb{E}[\epsilon_t^2 | \mathcal{F}_{t-1}]}_{=1}] \\
    &= \mathbb{E}[\sigma_t^2]
\end{align*}


\begin{align*}
    \sigma_t^2 &= \omega + \sum_{j=1}^h \beta_j \sigma_{t-j}^2 + \sum_{j=1}^k \alpha_j y_{t-j}^2 \\
    &= \omega + \sum_{j=1}^h \beta_j \sigma_{t-j}^2 + \sum_{j=1}^k \alpha_j \sigma_{t-j}^2 \epsilon_{t-j}^2 \\
    &= \omega + \sum_{j=1}^h \beta_j \left( \omega + \sum_{i=1}^h \beta_j \sigma_{t-j-i}^2 + \sum_{i=1}^k \alpha_i \sigma_{t-j-i}^2 \epsilon_{t-j-i}^2  \right) + \sum_{j=1}^k \alpha_j \epsilon_{t-j}^2 \left(\omega + \sum_{i=1}^h \beta_i \sigma_{t-j-i}^2 + \sum_{i=1}^k \alpha_i \sigma_{t-j-i}^2 \epsilon_{t-j-i}^2 \right) \\
    &= \omega + \left( \sum_{j=1}^h \beta_j + \sum_{j=1}^k \alpha_j \epsilon_{t-j}^2 \right)\omega + \omega\left(\sum_{j=1}^h \beta_j \left(\sum_{i=1}^h \beta_i + \sum_{i=1}^k \alpha_i \epsilon_{t-j-i}^2 \right) + \sum_{j=1}^k \alpha_j \epsilon_{t-j}^2 \left(\sum_{i=1}^h \beta_i + \sum_{i=1}^k \alpha_i \epsilon_{t-j-i}^2 \right) \right) + \cdots \\
    &= \omega\sum_{i=0}^\infty \mu(t,i)\\
    & \vdots \\
    & \text{where } \mu(t,0) = 1\\
    & \mu(t,1) = \sum^h\beta_j + \sum^k \alpha_j + \epsilon_{t-j}^2\\
    & \vdots\\
    &= \mu(t,k+1) = \sum^h \beta_j \mu(t-j),k) + \sum^k \alpha_j \epsilon_{t-j}^2 \mu(t-j,k) \\
    &\text{Note: } \mathbb{E}[ \mu(t,k)] = \mathbb{E}[\mu(s,k)] \quad \forall t,s,k, \text{because $\{\epsilon_t\}_t$ is stationary}\\
    & \rightarrow \mathbb{E}[\mu(t,k+1)] = \left(\sum^h \beta_j + \sum^k \alpha_j \underbrace{\mathbb{E}[\epsilon_{t-j}^2]}_{=1} \right)\mathbb{E}[\mu(t,k)] \\
    \Rightarrow \mathbb{E}[\sigma_t^2] &= \omega \sum_{i=0}^\infty \left(\sum_{j=1}^k \alpha_j  + \sum_{j=1}^h \beta_j \right)^i \underbrace{\mathbb{E}[\mu(t,0)]}_{=1} \\
    &= \omega\sum_{i=0}^\infty \left(\sum_{j=1}^k \alpha_j \sum_{j=1}^h \beta_j \right)^i\\
    &= \frac{\omega}{1- \left( \sum^k \alpha_j + \sum^h \beta_j \right)} \\
    &\text{Converges } \Leftrightarrow \sum^k \alpha_j + \sum^h \beta_j < 1
\end{align*}

$\Rightarrow$ GARCH(h,k) is a stationary model 
\[ \Leftrightarrow \sum^j \alpha_j + \sum^h \beta_j < 1\]
 

\subsubsection*{4. Estimation}

\textbf{Question:} We typically estimate GARCH models using maximum likelihood. 
\begin{enumerate}
    \item Recall the definition of the likelihood function and loglikelihood function (the latter being denoted by $\ell(\theta;y)$)
    \item Derive the loglikelihood function for the GARCH(h,k) model. Write down the exact steps how to calculate this function.
\end{enumerate}

\textbf{Answer:} \\

\textbf{1.}
\begin{itemize}
    \item \textbf{Loglikelihood Function:}
    \begin{itemize}
        \item The likelihood function $L(\theta;y)$ measures the probability of observing the given sample data $y$ under the parameters $\theta$ of a statistical model. For a set of observations $y_1,y_2,\ldots, y_T$ with density function $f(y_t;\theta)$, the likelihood function is: \[
        L(\theta;y)=\prod_{t=1}^T f(y_t;\theta)
        \]
    \end{itemize}
    \item \textbf{Log-likelihood Function:}
    \begin{itemize}
        \item The log-likelihood function $\ell (\theta;y)$ is the natural logarithm of the likelihood function. It simplifies the computation and is given by: \[
        \ell (\theta;y) = \log L(\theta;y)=\sum_{t=1}^T \log f(y_t;\theta)
        \]
    \end{itemize}
\end{itemize}
\textbf{2.} 
\begin{enumerate}
    \item \textbf{GARCH(h,k) Model Specification:}
    \begin{itemize}
        \item A GARCH model consists of two equations:
        \begin{itemize}
            \item Mean equation: \[
            y_t=\mu + \varepsilon_t
            \]
            \item Variance Equation: \[
            \sigma_t^2 = \alpha_0 + \sum_{i=1}^h \alpha_i \varepsilon_{t-i}^2 + \sum_{j=1}^k \beta_j \sigma_{t-j}^2
            \] where $\alpha_0>0, \alpha_i\geq 0, \beta_j \geq0$, and $\sigma_t^2$ is the conditional variance of $\varepsilon_t$
        \end{itemize}
    \end{itemize}
    \item \textbf{Assume $\varepsilon_t \sim N(0,\sigma_t^2)$:}
    \begin{itemize}
        \item Given the normal distribution assumption, the density function of $\varepsilon_t$ is: \[
        f(\varepsilon_t) = \frac{1}{\sqrt{2\pi \sigma_t^2}} \exp \left(-\frac{\varepsilon_t^2}{2\sigma_t^2}\right)
        \]
    \end{itemize}
    \item \textbf{Substitute $\varepsilon_t = y_t -\mu$:}
    \begin{itemize}
        \item \[
        f(y_t;\theta)=\frac{1}{\sqrt{2\pi \sigma_t^2}} \exp\left(-\frac{(y_t-\mu)^2}{2\sigma_t^2}\right)
        \] where $\theta$ represents the vector of parameters $(\mu,\alpha_0, \alpha_1,\ldots,\alpha_h,\beta_1,\ldots,\beta_k)$
    \end{itemize}
    \item \textbf{Write the Log-Likelihood Function:}
    \begin{itemize}
        \item The log-likelihood function for $y_i,y_2,\ldots,y_T$ is:\[
        \ell(\theta;y) = \sum_{t=1}^T \log f(y_t;\theta)
        \]
        \item Substitute the density function: \[
        \ell(\theta;y) = \sum_{t=1}^T \left[ -\frac{1}{2} \log (2\pi \sigma_t^2)-\frac{(y_t-\mu)^2}{2\sigma_t^2} \right]
        \]
    \end{itemize}
    \item \textbf{Simplify the Expression:}
    \[
    \ell(\theta;y) = -\frac{T}{2} \log(2\pi) -\frac{1}{2} \sum_{t=1}^T \log (\sigma_t^2)-\frac{1}{2} \sum_{t=1}^T \frac{(y_t-\mu)^2}{\sigma_t^2}
    \]
\end{enumerate}




\end{footnotesize}
