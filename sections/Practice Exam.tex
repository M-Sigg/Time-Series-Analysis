\subsection*{Question 1}

Let \(\{X_t\}\) denote a monthly time series. Then \(\nabla \nabla_4 X_t\) is given by:

\begin{itemize}
    \item[(a)] \(X_{t-4} - X_{t-5}\)
    \item[(b)] \(X_t - X_{t-1} - X_{t-4} + X_{t-5}\)
    \item[(c)] \(X_t - 2X_{t-4} + X_{t-8} - X_{t-1} + 2X_{t-5} - X_{t-9}\)
    \item[(d)] None of the above
\end{itemize}

\begin{footnotesize}
\begin{align*}
    \nabla \nabla_4 X_t &= \nabla(X_t - X_{t-4}) \\
    &= \nabla X_t - \nabla X_{t-4} \\
&= X_t-X_{t-1} -X_{t-4} + X_{t-5}\\
&\Rightarrow (b)
\end{align*}
\end{footnotesize}


\subsection*{Question 2}

Let $\{x_1, \ldots,x_T\}$ be generated by a SARIMA(1,0,0)$\times$(0,1,0)$_4$ process. Assuming that the parameter $\alpha$ of the AR(1) part is known, what is the 1-step ahead forecast?  

\begin{itemize}
    \item[(a)] $X_{T-3}+\alpha(X_T + X_{T-4}) $
    \item[(b)] $X_{T-3}+\alpha(X_T + X_{T-5}) $
    \item[(c)] $X_{T-4}+\alpha(X_{T-1} + X_{T-5}) $
    \item[(d)] $X_{T-3}+\alpha(X_T + X_{T-4}) $
\end{itemize}

\begin{footnotesize}
\begin{align*}
    \phi_1 (B) \nabla^1_4 X_t &= Z_t \\
    (1-\alpha B)(X_t-X_{t-4}) &= Z_t \\
    X_t-X_{t-4} - \alpha X_{t-1} + \alpha X_{t-5} &= Z_T\\
    X_t &= X_{t-4} + \alpha(X_{t-1} - X_{t-5}) + Z_t \\
    E[X_{T+1}|F_t] &= E[X_{T-3} + \alpha (X_t - X_{T-4}) + Z_{T+1} | F_T] \\
    &= X_{T-3} + \alpha (X_T - X_{T-4} ) \\
&\Rightarrow (d)
\end{align*}
\end{footnotesize}





\subsection*{Question 3}

Let $\{X_T\}$ denote a discrete stochastic process. Which of the following statements is true?

\begin{itemize}
    \item[(a)] Strict stationarity of $\{X_T\}$ always implies weak stationarity of $\{X_T\}$
    \item[(b)] Strict stationarity of $\{X_T\}$ implies weak stationarity of $\{X_T\}$ if $\mathbb{E}[X_t] < \infty, \quad \forall t$
    \item[(c)] Strict stationarity of $\{X_T\}$ implies weak stationarity of $\{X_T\}$ if $Var(X_t) < \infty, \quad \forall t$
    \item[(d)] None of the above
\end{itemize}

\begin{footnotesize}
$\Rightarrow (c)$. We need the additional condition that the second moments are finite. Strict stationarity only imposes the invariance of the joint distribution of the process, but does not say anything about the existence of first or second moments. For weak/ covariance stationarity, we also want to have finite first/ second order moments. Variance being finite implies that expectations are also finite. 
\end{footnotesize}



\subsection*{Question 4}

Let $X_t= 0.5 + 1.2X_{t-1} - 0.4 X_{t-2} + Z_t + 1.1 Z_{t-1} $. Which of the following statements is true? 

\begin{itemize}
    \item[(a)] $\{X_T\}$ is weakly stationary and invertible
    \item[(b)] $\{X_T\}$ is weakly stationary but not invertible 
    \item[(c)] $\{X_T\}$ is not weakly stationary but invertible
    \item[(d)] $\{X_T\}$ is neither weakly stationary nor invertible
\end{itemize}

\begin{footnotesize}
AR polynomial: 
\begin{align*}
    (1-1.2x + 0.4 x^2) &= 0 \\
    \rightarrow x &= \frac{1.2 \pm \sqrt{1.2^2 - 4 \cdot 0.4}}{2 \cdot 0.4}\\
    &= \frac{1.2 \pm i \sqrt{0.16}}{0.8} \\
    &= 1.5 \pm 0.5i \\
    \rightarrow \left| 1.5 \pm 0.5i \right| &= \sqrt{1.5^2 + 0.5^2} \\
    &= \sqrt{2.25 + 0.25} \\
    &= \sqrt{2.5} \\
    &= 1.58 \\
    \Rightarrow &\text{ stationary}
\end{align*}
MA polynomial:
\begin{align*}
    (1 + 1.1x) &= 0\\
    x &= \frac{1}{-1.1} < 1 \\
    \Rightarrow \text{ not invertible}
\Rightarrow (b)
\end{align*}

\end{footnotesize}

\subsection*{Question 5}

Let $X_t= 20 + 0.5X_{t-1} + 0.25 X_{t-2} + Z_t -0.5Z_{t-1}$ with $\sigma^2_Z = 1$. The unconditional expectation and variance of $X_t$ are:

\begin{itemize}
    \item[(a)] $\mu=2, \sigma_X^2=\frac{116}{25}$
    \item[(b)] $\mu=2, \sigma_X^2=\frac{21}{25}$ 
    \item[(c)] $\mu=2, \sigma_X^2=\frac{1}{25}$
    \item[(d)] None of the above
\end{itemize}

\begin{footnotesize}
    \textit{Answer:} \\
    1. Look at the polynomial $(1-0.5x-0.2x^2)=0$, where the roots are $-3.8$ and $1.3$ , thus the process is (weakly) stationary and we have $\mathbb{E}[X_t] = \mathbb{E}[X_{t-k}]$.
    \begin{align*}
        \mathbb{E}[X_t] &= 20 + 0.5\mathbb{E}[X_t] + 0.25\mathbb{E}[X_t] \Rightarrow \mathbb{E}[X_t]  = 80
    \end{align*}
    As none of the possible answer align with this $\Rightarrow (d)$
\end{footnotesize}

\subsection*{Question 6}

Consider the ARMA(1,1,1) process $(1-\alpha B)(1-B)X_t = (1-\theta B)Z_t$. Which of the following statements is correct?

\begin{itemize}
    \item[(a)] As long as $|\alpha|<1$, the process $X_t$ is stationary.
    \item[(b)] As long as $|\theta|<1$, the process $X_t$ is invertible.
    \item[(c)] As long as $|\alpha|<1$, the process $\nabla X_t$ is invertible.
    \item[(d)] As long as $|\theta|<1$, the process $\nabla X_t$ is stationary.
\end{itemize}

\begin{footnotesize}
    \textit{Answer:} \\
    (c) and (d) are clearly wrong, as it checks the AR part $\alpha$ for invertibility and the MA part $\theta$ for stationarity. \\

    From the AR polynomial $(1-\alpha B)(1-B)$ it is clear that this has unit root, thus the process is not stationary. So the answer is not (a). This leaves only (b), which we can confirm by looking at the MA polynomial $(1-\theta x) = 0$. We have $x= \frac{1}{\theta}$, which is $<1$ as long as $|\theta| < 1$.
\end{footnotesize}

\subsection*{Question 7}

Consider the AR(2) process $X_t = 1.5 + 0.8X_{t-1}- 0.4 X_{t-2} + Z_t$. Find $\rho_X(3)$, rounded to two significant digits.

\begin{itemize}
    \item[(a)] $\rho_X(3)=0.12$
    \item[(b)] $\rho_X(3)=-0.21$
    \item[(c)] $\rho_X(3)=-0.18$
    \item[(d)] $\rho_X(3)=0.24$
\end{itemize}

\begin{footnotesize}
    \textit{Answer:} \\
    Setup Yule-Walker equations:
    \begin{align}
        \rho_X(3) &= 0.8\rho_X(2) -0.4\rho_X(1)\label{corr3} \\
        \rho_X(2) &= 0.8\rho_X(1)- 0.4\rho_X(3) \label{corr2} \\
        \rho_X(1) &= 0.8 \rho_X(0)- 0.4 \rho_X(1) \label{corr1}
    \end{align}
    
    From (\ref{corr1}) we get: $1.4 \rho_X(1)=0.8\underbrace{\rho_X(0)}_{=1} \Rightarrow \rho_X(1)=0.57$. Inserting this into (\ref{corr2}) we get: $\rho_X(2)=0.8*0.57 -0.4=0.056$. And again inserting this into (\ref{corr3}) we get: $\rho_X(3)=0.8 * 0.056-0.4*0.57 = -0.18$.\\

    Thus the answer is $(c)$
\end{footnotesize}


\subsection*{Question 8}

Consider the ARMA(2,1) process $X_t = 0.5X_{t-1}-0.2X_{t-2}+Z_t -0.1Z_{t-1}$. Assume the model coefficients are known and that $\sigma_z^2=2$. Find the variance of the forecast error $e_T(3)=X_{T+3}-\hat{X}_{T+3}$.

\begin{itemize}
    \item[(a)] $Var(e_T(3))=1.07$
    \item[(b)] $Var(e_T(3))=1.16$
    \item[(c)] $Var(e_T(3))=1.52$
    \item[(d)] None of the above
\end{itemize}

\begin{footnotesize}
    \textit{Answer:} 1. Express $\hat{X}_{T+3}$ in terms of known values (i.e., $X_T,X_{T-1}, ...$): 
    \begin{align*}
        \hat{X}_{T+3} &= \mathbb{E}[X_{T+3}|F_T]\\
        &= \mathbb{E}[0.5X_{T+2}-0.2X_{T+1}+\cancel{Z_T+3} -\cancel{0.1Z_{T+2}}|F_T] \\
        &= 0.5 \mathbb{E}[0.5X_{T+1} - 0.2 X_{T} + \cancel{Z_{T+2}} - \cancel{0.1Z_{T+1}}|F_T]- 0.2\mathbb{E}[X_{T+1}|F_T] \\
        &= 0.05 \mathbb{E}[X_{T+1}|F_T]-0.1X_T\\
        &=0.05\mathbb{E}[0.5X_T-0.2X_{T-1}+\cancel{Z_{T+1}}-0.12Z_T|F_T]-0.1X_T\\
        &= -0.075X_T -0.01X_{T-1}-0.005Z_T
    \end{align*}
    2. Also express $X_{T+3}$ in terms of known values: 
    \begin{align*}
        X_{T+3} &= 0.5X_{T+2}-0.2 X_{T+1} + Z_{T+3} -0.1Z_{T+2} \\
        &= 0.5(0.5X_{T+1}-0.2X_T + Z_{T+2} -0.1Z_{T+1}) -0.2X_{T+1} + Z_{T+3} -0.12Z_{T+2} \\
        &\vdots \\
        &= -0.075X_T-0.01X_{T-1} + Z_{T+3} +0.4 Z_{T+2} -0.05Z_{T+1} -0.005Z_T
    \end{align*}
    3. Calculate the forecast error: \begin{align*}
        X_{T+3}-\hat{X}_{T+3} = Z_{T+3} + 0.4Z_{T+2}-0.05 Z_{T+1}
    \end{align*}
    4. Calculate the variance of that:
    \begin{align*}
        \mathbb{E}[(Z_{T+3} + 0.4Z_{T+2}-0.05 Z_{T+1})^2] &= 2 + 0.4^2\cdot2 + 0.05^2\cdot2\\
        &=2.325 \\
        &\Rightarrow (d)
    \end{align*}
\end{footnotesize}


\subsection*{Question 9}

Consider the MA(2) process $X_t = Z_t +0.5 Z_{t-1} + 0.25 Z_{t-2}$, where $\sigma_Z^2=1$. The spectral density of this process equals:

\begin{itemize}
    \item[(a)] $f(\omega)= \frac{1}{2\pi}\left[\frac{21}{16} + 1.25\cos{\omega} + 0.5\cos{2 \omega} \right]$
    \item[(b)] $f(\omega)= \frac{1}{2\pi}\left[\frac{1}{16} + 1.25\cos{\omega} + 0.5\cos{2 \omega} \right]$
    \item[(c)] $f(\omega)= \frac{1}{2\pi}\left[2 + 0.25\cos{\omega} - 0.5\cos{2 \omega} \right]$
    \item[(d)] None of the above
\end{itemize}


\begin{footnotesize}
    \textit{Answer:} The spectral density is given by:\[
    f(\omega)= \frac{1}{2\pi}\left[\gamma(0) + 2 \sum_{j=1}^\infty \gamma(j) \cos{\omega j} \right]
    \]
    We can calculate $\gamma(0)$ as follows:
    \begin{align*}
        \gamma(0)&=\mathbb{E}[(X_t- \underbrace{\mathbb{E}[X_t]}_{=0})^2] \\
        &= \mathbb{E}[(Z_t + 0.5Z_{t-1} + 0.25Z_{t-2})^2] \\
        &= 1 + 0.25 + 0.0625 = 1.3125 \\
        &= \frac{21}{16}
    \end{align*}
    We can then need $\gamma(1), \gamma(2)$ ($\gamma(k)=0 \forall k>2$):
    \begin{align*}
        \gamma(1) &= 0.5Z_t^2 + 0.5\cdot0.25 Z_{t-1}^2 = 0.625\\
        \gamma(2)&= 0.25Z_t^2 = 0.25
    \end{align*}
    Inserting everything into $f(\omega)$:
    \begin{align*}
        f(\omega)&=\frac{1}{2\pi} \left[\frac{21}{16} + 1.25 \cos{\omega} + 0.5\cos{2\omega} \right] \\
        &\Rightarrow (a)
    \end{align*}
    \textit{Note:} The provided solution say (d) is correct, because the normalization factor $\frac{1}{2\pi}$, is wrong and it should be $\frac{1}{\pi}$, but I don't know why that would be the case.  
\end{footnotesize}

\subsection*{Question 10}

Let $X_t = Z_t$ be a white noise process with $\sigma_Z^2=1$. What is the fraction of variance explained by the frequencies in $[0.25\pi, 0.375\pi]$?

\begin{itemize}
    \item[(a)] 50\%
    \item[(b)] 27.5\%
    \item[(c)] 12.5\%
    \item[(d)] None of the above
\end{itemize}

\begin{footnotesize}
    1. Calculate spectral density $f(\omega)$. This is just $\frac{1}{2/pi}$, as the variance is just $1$ and there is no autocovariance.\\
    2. Integrate $\int_{0.25\pi}^{0.375\pi} f(\omega)$. This is straight forward with the given white noise process and is simply: $0.375\pi-0.25\pi=0.125$\\
    3. Express this as a fraction of the total variance: $\frac{0.125}{1}= 12.5\%$. Thus $(c)$ is the correct answer.
\end{footnotesize}

\subsection*{Question 11}

Consider the ARMA-GARCH model \begin{align*}
    X_t &= 0.5+0.25 X_{t-1} + 0.125 X_{t-2} + Y_t \\
    Y_T &= \sigma_t \epsilon_t, \quad \epsilon_t \sim N(0,1)\\
    \sigma^2_t = 0.5\sigma_{t-1}^2 + 0.25 \sigma_{t-2}^2 + 0.125 Y_{t-1}^2 
\end{align*}
Which of the following statements is correct?

\begin{itemize}
    \item[(a)] The process is weakly stationary.
    \item[(b)] The process is not weakly stationary, since only the ARMA part is not stationary.
    \item[(c)] The process is not weakly stationary, since only the GARCH part is not stationary.
    \item[(d)] The process is not weakly stationary, since both the ARMA and GARCH parts are not stationary.
\end{itemize}

\begin{footnotesize}
    ARMA part: \begin{align*}
    (1-0.25x-0.125x^2) &= 0 \\
    x &= -4, x= 2 > 1 \\
    &\Rightarrow \text{stationary}
    \end{align*}
    GARCH part: \begin{align*}
        0.5 + 0.25 + 0.125 &= 0.875 < 1\\
        &\Rightarrow \text{Stationary}
    \end{align*}
    $\Rightarrow (a)$
\end{footnotesize}



\section{Short Answer Practice Questions}
\textit{
These are my personal answers, there are no provided solutions.}

\subsection*{Question 1}

If a series $X_t$ and another series $Y_t$ are both strictly stationary, is their sum $X_t+Y_t$ also always strictly stationary? Why or why not? \\

\begin{footnotesize}
    \textit{Answer:} Yes, if two series are both strictly stationary, their sum is also strictly stationary. This follows from the properties of the joint distribution of the strictly stationary series. 
\end{footnotesize}

\subsection*{Question 3}

In the TAR(1) model is $\mathbb{E}[X_t |X-{t-1}]$ always closer to the unconditional mean than $X_{t-1} $ is? Why or why not? \\

\begin{footnotesize}
\textit{Answer:} If the conditional mean $\mathbb{E}[X_t |X-{t-1}]$ is closer to the unconditional mean depends on the regimes of the threshold autoregressive model. Thus it is generally not true. Consider the following counterexample. \\

\begin{subequations}
\begin{empheq}[left = {X_t=\empheqlbrace}]{align*} 
    &X_T = 1.001 X_{t-1} &&\text{if $X_{t-1} \leq 100 $} \\
    &X_t = 100 &&\text{if $X_{t-1}> 100 $}
\end{empheq}
\end{subequations}

In this case, when $X_{t-1} > 100$, the values for $X_t$ is always $100$, and $\mathbb{E}[X_t|X_{t-1}]=100$. This means that the conditional mean $\mathbb{E}[X_t|X-{t-1}]=100$ is the same as $X_{t-1}$ and thus not closer than to the unconditional mean than $X_{t-1}$. 
\end{footnotesize}


\subsection*{Question 4}

Let $X_t = 1 + b e_{t-2} + ce_{t-1} + e_t$. If a researcher estimates the $a$ with the sample mean of $X_t$ and wishes to calculate a confidence interval for $a$, is it better to use HAC with bandwidth (lag) = 0, bandwidth = 2 of bandwidth = 10? \\

\begin{footnotesize}
    \textit{Answer:} HAC estimators are used to adjust standard errors in the presence of heteroskedasticity and autocorrelation. The bandwidth determines the maximum lag of autocorrelation that is accounted for when estimating the covariance matrix. As the given model includes error terms for up to lag 2, the appropriate bandwidth is also at least 2.
\end{footnotesize}

\subsection*{Question 5}

Suppose that returns $r_t$ on an asset follow an i.i.d. process with mean zero. After one observation, that is a sample of length $T=1$, is $r^2-t$ an unbiased estimate of $var(r_t)$? Why or why not? In this situation, is unbiasedness enough to conclude that $r_t^2$ is a reliable estimate? Why or why not? \\

\begin{footnotesize}
    \textit{Answer:} $\mathbb{E}[r_1^2] = \mathbb{E}[r_t^2] = \sigma^2 = var(r_t)$. Thus, $r_1^2$ is an unbiased estimator of $var(r_t)$. \\

    Unbiasedness alone is not sufficient to conclude that it is a reliable estimate. Despite being unbiased, the estimate $r_1^2$ is based on a single observation, which means it has high variance and does not provide sufficient information to be considered reliable. 
\end{footnotesize}

\subsection*{Question 6}

Let $X_t = 1.7t^2 + 0.3 e_{t-1} + e_t$. Can $X_t$ be expressed as the sum of a stationary process $Z_t$ and a deterministic process $W_t$? If yes, do so, if not, why not? How would this help one construct forecasts? Is the problem different if $X_t = 1.7t^2 + 0.3X_{t-1} + e_t$? \\

\begin{footnotesize}
    \[
    X_t = 1.7t^2 + 0.3 e_{t-1} + e_t
    \] The term $1.7t^2$ is a deterministic process $W_t$ since it only depends on time $t$ and not on any random components. The terms $0.3e_{t-1} +e_t$ represent a stationary process $Z_t$. $e_t$ is white noise and thus $0.3e_{t-1} + e_t$ is a linear combination of white noise terms, which is also stationary. \\

    Hence, we can write it as $X_t = W_t + Z_t$. This helps forecasting because the deterministic part follows a known function $1.7t^2$. And the stationary part has an expected value of 0, making it predictable.\\

    In the second case: \[
    X_t = 1.7t^2 + 0.3X_{t-1} + e_t
    \] We again have the deterministic part $1.7t^2$. For the other part we can calculate the roots of the AR polynomial to confirm that is stationary $(x\approx 3.33 > 1)$. Thus we can once again write this process as the sum of the deterministic part $W_t$ and a stationary part $Z_t$.
\end{footnotesize}

\subsection*{Question 7}

Suppose that a researcher is working with an AR(1) model with no intercept where $X_t=bX_{t-1}+e_t$ and $e_t$ are i.i.d. with $N(0,1)$ and $|b|<1/2$. The researcher collects a sample time series $X_t$ but waits to start collecting until $X_1=1$ and stops collecting at $T$ before $X_t$ turns negative, i.e., tie $X_t>0, X_{T+1}\leq 0$. Choose and discuss briefly an implication of this sampling scheme for maximum likelihood estimation on $b$. \\

\begin{footnotesize}
    \textit{Answer:} Such a sampling scheme introduces bias into the data collection process because it selectively collects data only when $X_t$ is positive. This truncation of the data results in a sample that is not representative of the entire distribution of the AR(1) process. As a result, when estimating the parameter $b$ using maximum likelihood estimation, the estimator will be biased. The usual properties of the MLE, such as unbiasedness, consistency, and asymptotic normality, may not holds because the sample does not accurately reflect the underlying process. This biased sampling affects the accuracy of parameter estimates, confidence intervals, and hypothesis tests, making them unreliable.
\end{footnotesize}