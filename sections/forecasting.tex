\textbf{\underline{Problem:}}\\
Given an observed time series $X_1, X_2,\ldots,X_T$:\\
\begin{itemize}
    \item  Want to estimate/predict $X_{T+h}$, where $h$ is called the \textbf{time lead} or \textbf{forcasting horizon}
    \item Understand the margin of error for your prediction 
\end{itemize}

\textbf{Two Main Approaches}
\begin{itemize}
    \item Model Free Methods
    \item Model Based Methods
\end{itemize}


Summary: Decompose Time Series $X_1,\ldots,X_T$ into trend $f(t)$ and s stationary component $Z_t$. Extrapolate the trend and $Z_t$ without or with estimating a time series model (e.g., ARMA)

\subsection{Linear Model Free Methods}

'Model free' methods do not rely on fitting a specific statistical model, such as ARMA or SARIMA, to the data. Instead, these methods use simpler techniques to forecast future values based on the observed data. This might involve the \textbf{extrapolation of trend curves} from non-seasonal data or using methods such as \textbf{exponential smoothing}. \\

\underline{Example 1. Exponential Smoothing}\\

This should only be used for non-seasonal time series showing \textit{no} systematic trend. \\

Given such a time series $X_1,\ldots,X_T$, we want to predict $X_{T+1}$. We call this prediction $\hat{X}_{T+1}$. We write $\hat{X}_{T+1} $ as a weighted average of $X_1,\ldots,X_T$ with exponentially decaying weights \[
    \hat{X}_{T+1} = \theta(cX_t+c^2X_{T-1} + c^3X_{T-2}+\cdots+c^TX_1)
    \] for some $c^t$ with $0<c<1$ and $0<\theta<1$, and where the weight for $X_{T-1}$ is smaller than the weight of $X_T$.\\

We use a constant weight $0<c<1$. Thus the weights are a \textit{geometric series}. Now we face the question: How to choose $\theta$ given our $c$? One approach is to make sure all weights sum up to 1. This makes $\hat{X}_{T+1}$ a "genuine weighted average":
\begin{align*}
    \theta \sum_{k=1}^T c^k =1 \Rightarrow \theta \approx \frac{1-c}{c}
\end{align*}
This ensures the total weight is 1. \\


Equivalently, we can rewrite this in terms of $\alpha=1-c$:
\[
        \hat{X}_{T+1}= \alpha X_T+\alpha(1-\alpha)X_{T-1}+\alpha(1-\alpha)^2 X_{T-1}+\cdots 
\]
This simplifies to:
\[
        \hat{X}_{T+1}=\alpha X_T+(1-\alpha)\hat{X}_T\]
Where $e_T=X_T-\hat{X}_T$ is the prediction error at time $T$:\[
        X_{T+1}=\alpha e_T + \hat{X}_T \rightarrow \]

We can generate predictions for any $\alpha$ with $0<\alpha<1$. But, which $\alpha$ is the best? We choose $\alpha$ to minimize a sum of squares criteria:
\[min \sum_{T=1}e_t (\alpha)^2\] 
where $e_t(\alpha)=X_t-(\alpha X_{t-1}+ \alpha(1-\alpha)X_{t-2} +\cdots) $. Here let $e_t(\alpha)$ be the error you would have gotten using $\alpha$ as the smoothing parameter.\\

We can extend this to account for seasonality and trend:
\begin{align*}
            \hat{X}_{T+1} &= \alpha e_T+\hat{X}_T\\
            \text{to (A) } \hat{X}_{T+1}&= \alpha e_T+\hat{X}_T+\gamma(\text{Trend Increment}) \\
            \text{-or- (B) } \hat{X}_{T+1}&= \alpha e_T+\hat{X}_T+\gamma(\text{Trend Increment}) + \nabla(\text{Seasonality})
        \end{align*} 
        Use Holt or Holt-Winter procedure respectively. In R there is a package that does this. \textit{Note}: Holt and Holt-Winter are not exactly identical to (A), and (B).\\


To increase our forecast horizon we \fbox{use recursion:} 
    \begin{align*}
        \hat{X}_{T+2} &= \alpha e_{T+1} + \hat{X}_{T+1}\\
        &= \alpha(X_{T+1}-\hat{X}_{T+1})+\hat{X}_{T+1}
    \end{align*}







\subsection{Model-Based Prediction}

Model-based methods involve fitting a statistical model to the data, capturing the underlying patterns and structures, such as autocorrelation and seasonality. Once the model is fitted, it is used to generate forecasts based on the estimated parameters. \\

\underline{Example: $AR(2)$ model-based predictions}\\

Given an AR(2) model: \[X_t=\mu +\alpha_1X_{t-1} + \alpha_2 X_{t-2}+\varepsilon_t\] with $\varepsilon$ iid, mean zero and we know or estimated the parameters $\alpha_1, \alpha_2, \mu$, the prediction is:\[
\hat{X}_{T+1}=\mu +\alpha_1 X_T + \alpha_2 X_{T-1}
\]
The prediction has following properties: 

    \begin{align*}
        \mathbb{E}[\hat{X}_{T+1} | X_T, X_{T-1}] &= \mathbb{E}[\mu+\alpha_1 X_T+\alpha_2 X_{T-1} |X_T, X_{T-1}]\\
        &=\mathbb{E}[X_{T+1}-\varepsilon_{T+1} | X_T, X_{T-1}]\\
        &=\mathbb{E}[X_{T+1} | X_T, X_{T-1}]
    \end{align*}
$\Rightarrow$ $\hat{X}_{T+1}$ and $X_{T+1}$ have the same conditional expectations \\

Model-based predictions in general ARIMA models follow the same approach, using earlier estimates recursively. \\

\underline{Some other examples: $ARMA(1,1)$} \\

If we have an ARMA(1,1) model: 
\[X_t=\mu + \alpha X_{t-1}+\theta \varepsilon_{t-1}+\varepsilon_t \]
\textit{Note:} here, have an estimate of $\hat{\varepsilon}_{t-1} $ given $X_1,\ldots,X_T$ if you start with assumption $\varepsilon_0=0$ \\

In practice, the parameters of the model ($\mu,\alpha_1,\alpha_2$ in AR(2), $\mu,\alpha,\theta$ in ARMA(1,1)) are not known and need to be estimated. Additionally, the correct model must first be determined. 


\subsubsection{Procedure}

To make model-based predictions:
\begin{enumerate}
    \item Select a model using criteria such as BIC, AIC, or others.
    \item Fit the model parameters.
    \item Generate predictions by plugging in these parameters.
\end{enumerate}
This process is known as the \textbf{Box-Jenkins Procedure}. \\

\underline{Example}: with AR(1) $X_t=\mu+\alpha X_{t-1}+\varepsilon_t$ \\

We use maximum likelihood estimation to obtain $\hat{\mu}, \hat{\alpha}$. Then, we set: \[\hat{X}_{T+1}=\hat{\mu}+\hat{\alpha} X_T\]
For $h$-step ahead forecasts, recursion is used: 

        \begin{align*}
            \hat{X}_{T+2} &= \hat{\mu} +\hat{\alpha} \hat{X}_{T+1}\\
            \hat{X}_{T+3} &= \hat{\mu} +\hat{\alpha} \hat{X}_{T+2}\\
        \end{align*}

\subsection{Exercises}

\underline{Book 5.2}: \\

\begin{footnotesize}
Calculating forecast $\hat{X}_{T+h}$:
\begin{align*}
   &\text{Model: } X_t = \alpha X_{T-1} + Z_T \\
   &\text{Need to show: } \mathbb{E}[X_{T+h}|F_T] = \alpha^h X_T \\
   &\text{\quad where $F_T$ is the information set at time $T$} \\
   &\text{Insert model: } \mathbb{E}[\alpha X_{T+h-1} + Z_{T+h}|F_T] \\
   &\text{As $Z_{T+h}$ is independent of $F_T$} \\
   &\text{ \quad and $\mathbb{E}[Z_{T+h}]=0$ (white noise process). It drops out} \\
   &\Rightarrow \mathbb{E}[\alpha X_{T+h-1|F_T}] \\
   &\text{Insert model again: } \mathbb{E}[\alpha(\alpha X_{T+h-2} + Z_{T+h-1}) |F_T] \\
   &\text{Do this iteratively until $X_{T+h-i} = X_T$, i.e., $i=h$} \\
   &\Rightarrow \mathbb{E}[\alpha^h X_T|F_T] \\
   &\text{As $X_T$ is included in the information set we can take it out:}\\
   &\Rightarrow \hat{X}_{T+h}= \alpha^h X_T
\end{align*}

Calculating \textit{$h$-steps-ahead} forecast error ($X_{T+h}-\hat{X}_{T+h}$):
\begin{align*}
    & Var(X_{T+h}-\hat{X}_{T+h}) \\
    &= \mathbb{E}[(X_{T+h} - \mathbb{E}[X_{T+h}|F_t])^2 ] \\
    &= \mathbb{E}[(\alpha X_{T+h-1} +Z_{T+h} - \alpha^h X_T)^2 ] \\
    &\text{Iteratively inserting for $X_{T+h-i}$ until $i=h$ }\\
    &= \mathbb{E}[(\cancel{\alpha^h X_T} + Z_{T+h} +\alpha Z_{T+h-1} + \alpha^2 Z_{T+h-2} +\cdots - \cancel{\alpha^h X_T} )^2 ]\\
    &\Rightarrow E\left[\left(\sum_{j=0}^{h-1} \alpha^j Z_{T+h-j} \right)^2 \right] \\
    &\text{All $Z_t$ are identically distributed, thus: } E \left[ \sigma_z^2 \sum_{j=0}^{h-1} \alpha^{2j}  \right] \\
    &\text{Geometric Series } \Rightarrow E \left[ \sigma_z^2 \left( \frac{1-\alpha^{2h}}{1-\alpha^2} \right) \right] \\
    & \Rightarrow \sigma_z^2 \frac{1-\alpha^{2h}}{1-\alpha^2}
\end{align*}
\end{footnotesize}
